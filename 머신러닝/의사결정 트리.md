# 의사결정 트리

1. Decsion Tree
2. 분류, 회귀작업에 사용되며, 복잡한 데이터 세트도 학습할 수 있음
3. 설명하기 쉽고, 해석하기 쉽고, 나이브베이즈와 같이 범주형 데이터를 잘 다를수 있음
4. 의사결정 트리는 강력한 머신러닝 알고리즘 가운데 하나인 랜덤포레스트의 기본구성 요소
   1. 랜덤포레스트 : 부트스트랩 어그리게이팅이라고 하는 배깅방식 사용
5. ID3, C4.5, CART, CHAID 와 같은 알고리즘이 있음



## 의사결정 트리의 특징

- 데이터를 분할하기 위해 가장 중요한 특징이 무엇인지 선택하는 로컬 최적화 방법을 계속 적용하는 탐욕적(greed) 방식으로 트리 생성
- 의사결정 트리는 학습 샘플을 서브 셋으로 분할하면서 생성되며, 분할의 과정은 각 서브 셋에 대해 재귀형태로 진행됨
- 각 노드에서의 분할은 특징 값을 기반으로 조건 검사를 통해 진행되며, 서브셋이 동일한 클래스 레이블을 가지는 경우나 분할을 통한 클래스 분류가 더이상 의미가 없을 경우 분할 작업을 마침



## CART 알고리즘 특징

- Classfication and Regression Tree

- 노드를 왼쪽, 오른쪽 자식노드로 분할 확장하며 트리를 생성함

- 분할 단계에서 가장 중요한 특징과 해당 값의 모든 가능한 조합을 측정 함수를 이용해 탐욕적으로 찾음

- 범주형 특징의 경우 해당 특징 값을 가진 샘플을 오른쪽 자식 노드에 할당함

- 수치형 특징의 경우 해당 값보다 큰 값을 가진 샘플들을 오른쪽 자식노드에 할당함

- 트리분할 측정 기준

  - 지니 불순도 : Gini Impurity

  - 한노드에 모든 샘플이 같은 클래스에 있을 경우 순수(gini == 0)하다고 함

    (마지막노드(립노드)에 가면 지니계수가 0이 됨)

  - 맵 스탭스의 파라미터 조정으로 depth 분할을 정지시킬수 있음

  - CART가 분류에 대해 최소화해야 하는 비용함수

  - 훈련 세트의 분할은 최대 깊이가 되면 중지하거나, 샘플의 개수가 분할에 필요한 최소 개수 이하일 경우나, 불순도를 줄이는 분할을 찾을 수 없을 때 중지됨



## 의사결정 트리의 특징 중요도

- 의사결정 트리를 기반으로 하는 모델은 특징의 상대적 중요도를 측정하기 쉬운 장점을 지님
- 의사결정 트리는 일부 특징은 배제함
- 의사결정 트리의 각 노드에 사용된 특징 중요도
- 각 노드에 사용된 특징 중요도의 합이 1이 되도록 전체 합으로 나누어 정규화함



## 분류 모델의 성능 측정 방법

### 혼동 행렬

- 실제 관측값을 얼마나 정확하게 예측했는지 보여주는 행렬
- True Positive : 참-긍정; 예측: 참, 실제: 참 일치
- True negative : 참-부정; 예측: 참, 실제: 참 일치
- False Positive : 거짓-긍정; 예측:거짓, 실제 : 참 불일치 
- False negative : 거짓-부정; 예측:거짓, 실제 : 참 불일치
<br><br>
- 정확도 : accuracy. 전체 샘플에서 정확하게 예측한 샘플 수의 비율
- 정밀도 : precision. Positive 클래스로 예측한 샘플에서 실제 Positive클래스에 속하는 샘플
- 재현율 : recall. 실제 Positive클래스에 속한 샘플 Positive 클래스에 속한다고 예측한 샘플 수의 비율
- 위양성율 : fallout. 실제 Positve클래스에 속하지 않는 샘플에서 Positive클래스에 속한다고 예측한 샘플 수의 비율
<br> 거짓 긍정률: FalsePostiveRate
- F1 점수: f1 score. 정밀도와 재현율의 조화평균. 점수에 상수2를 곱해 정밀도와 재현율이 모두 1일 경우 점수가 1이 되도록 만듬
- 특이성 : specificity. 실제 Negative 클래스에 속한 샘플에서 Negative클래스에 속한다고 예측한 샘플 수의 비율(1-FPR)
- AUC: area under the curve. 곡선하 면적
- 참 긍정률TPR과 거짓 부정률FPR사이를 표현하기 위해 ROC커브를 사용
- 예측된 확률로 부터 여러 클래스로 분류를 수행하는 데 활용

> ROC 커브가 만들어지도록 모델 개선해야 함



## 앙상블 학습과 배깅

- 디시젼 트리의 가장큰 문제는 훈련데이터에 대한 고분산성을 갖음
- 앙상블은 성능이 좋지 않은 알고리즘들을 결합시켜 종합한 결과 더 좋게 만드는 것(대표 케이스 배깅)
- 부트스트랩: Bootstrap. 중복을 허용하는 리샘플링
- 배깅 : Bagging. Bottstrap aggregating의 약자
    - 훈련 데이터에서 부트스트래핑한 샘플에 대해 모든 변수를 선택해 다수의 의사결정 트리를 구성
    <br>(의사결정 트리의 분할을 위해 선택된 변수가 모든 트리에서 대체로 비슷해져 상관관계를 가지고 종합을 해도 분산 감소효과가 기대만큼 크지 않을 수 있음)
    - 하나의 예측기를 위해 같은 훈련 샘플을 여러번 샘플링 할 수 있음
    - 동시에 CPU의 멀티코어나 병렬 프로세서에서 학습을 진행하는 모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아 새로운 샘플에 대한 예측을 만듬
    - 개별 예측기의 편향은 높지만, 수집함수를 통과한 후 앙상블의 결과는 원본 데이터 셋으로 하나의 예측기를 훈련시킬 때보다 편향은 비슷하지만 대체로 분산은 줄어듬



## 랜덤포레스트
- 랜덤 포레스트는 특징 기반 배깅 방법을 적용한 의사결정 트리의 앙상블
> 특징기반 배깅 : 특징을 선택적, 무작위적으로 사용함
- 트리 배깅은 의사결정 트리 모델의 단점 중 하나인 고분산을 줄여주며 이를 통해 단일 트리보다 훨씬 더 좋은 성능을 제공함
- 개별 트리간의 상관관계 문제를 피하기 위해 부트스트래핑 과정에서 훈련 데이터로부터 전체 p개의 변수중 m개의 변수만 선택
- 랜덤 포레스트의 무작위성 주입은 트리를 보다 다양하게 만들고, 편향을 손해 보는 대신, 분산을 낮추어 훌륭한 모델을 만듬
<br><br>
### 특징 중요도
- 의사결정 트리를 기반으로 하는 모델이므로 특징의 상대적 중요도를 측정하기 쉬운 장점
- 무작위성이 주입된 랜덤포레스트 모델은 의사결정 트리와 달리 모든 특징에 대해 중요도를 측정함
- 랜덤포레스트의 특징 중요도는 각 의사결정 트리의 특징 중요도를 모두 합한 후 트리의 수로 나눈것으로, Scikit-Learn의 경우 중요도의 합이 1이 되도록 결과값을 졍규화 함

## 랜덤 포레스트의 성능 개선을 위한 주요 파라미터(=하이퍼파라미터 : 우리가 지정해서 써야함)
- max_features : 최적의 분할 지점을 찾기 위해 검토할 특징의 개수. 일반적으로 n차원의 데이터세트의 루트n의 반올림 값을 설정
- n_estimator : 트리의 개수가 많을 수록 성능이 좋지만 계산시간이 많이 걸림. 일반적으로 100, 200, 500을 설정(500이상은 차이가 거의 없음)
- min_sample_splits : 노드에서 추가 분할을 위해 필요한 샘플의 최소 개수. 숫자가 너무 작으면 오버피팅, 너무 크면 언더피팅, 일반적으로 10, 30, 50 으로 시작
