# 나이브 베이즈

- 확률 기반 분류기
- 데이터가 각 클래스에 속할 특징 확률을 계산
- 나이브(Naive) : 예측한 특징이 상호 독립적이라는 가정 하에 확률 계산을 단순화함
- 베이즈(Bayes) : 입력 특징이 클래스 전체의 확률 분포 대비 특정 클래스에 속할 확률을 베이즈 정리를 기반으로 계산
  - 베이즈 정리에서 P(A|B)는 사건 B가 주어졌을 때 사건 A가 일어날 확률
  - 나이브 베이즈의 목표는 n개의 특징을 가진 샘플 데이터 x가 주어졌을때 k개의 클래스 중 하나에 속할 확률을 결정하는 것
  - 샘플 데이터는 x1~xn 의 값을 가진 특징으로 구성되고, yk는 샘플 데이터가 k에 속하는 사건을 나타냄, 베이즈 정리를 적용
- 사전확률 : 샘플데이터가 k에 속하는 사건의 확률p(yk)는 관측값의 특징에 대한 지식 없이 클래스가 어떻게 분포되어 있는지 나타냄
- 사전확률은 사전에 결정 되어 있거나, 학습 샘플 데이터를 이용해 학습시킬 수도 있음
- p(yk|x)는 관측값에 대해 외부 지식을 이용한 "사후 확률"
- 유사가능도 : p(x|yk)는 클래스 yk에 속한 샘플 데이터가 주어졌을 때 n개의 특징에 대한 결합 분포를 나타냄
- 나이브베이즈는 특징 간 서로 독립이라는 가정이 있으므로, n개의 특징에 대한 결합 조건부 분포는 특징들의 분포를 곱한 것으로 표 현할 수 있음
- p(x)는 특정 클래스에 속하지 않은 특징의 분포에 따라 값이 계산되므로 상수로 처리 할 수 있음
- 사전 확률과 유사 가능도를 이용해 사후 확률을 계산할 수 있음
- 라플라스 스무딩: Laplace smoothing 특징의 출현 횟수 초기값을 1부터 시작해 0을 곱해 발생하는 문제 해결(발견되지 않은 특징의 출현 빈도 초기값을 1로 설정)





## 주요내용 정리

1. 나이브 베이즈 분류기는 확률기반 분류기로 특징이 상호 독립적이라는 가정 하에 클래스 전체의 확률 분포 대비 특정 클래스에 속할 확률을 베이즈 정리를 기반으로 계산함
2. CountVectorizer를 사용해 불용어 제거와 단어의 출현 빈도 특징을 추출할 수 있다
3. 벡터 변환 시 훈련 데이터에 대해서는 fit_transform()을 사용하고, 테스트 데이터에 대해서는 transform() 메서드를 사용 한다
4. 나이브 베이즈 분류기는 특징의 출현 빈도 계산용 초기값 스무딩 파라미터 alpha와 학습 데이터에 대해 사전 확률 사용을 결정하는 파라미터 fit_prior로 파라미터 튜닝을 할 수 있다

