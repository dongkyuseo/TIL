# 단순선형회귀



- 회귀의 목표는 연속형 반응 변수의 값을 예측하는 것
- 단순 선형 회귀란 설명 변수인 단일 특징과 단일 반응 변수 간에 선형 관계가 있다고 가정하고 초평면이라고 하는 선형 평면을 이용해 모델링 한 것
- 단순 회귀는 설명 변수의 차원과 반응 변수의 차원, 모두 2개의 차원을 가지며, 초평면은 한차원 낮은 1차원이 됨(1차원의 초평면은 선)

> y = 알파 + 베타x



- y : 반응 변수의 예측 값
- 절편항 알파와 계수 베타 : 알고리즘을 통해 학습하게 되는 모델의 파라미터
- x : 설명 변수

> 알고리즘을 이용해 학습하면 최적의 알파 베타 값을 찾아냄



### 최소 제곱법

- OLS(Ordinary Leat Squares) 또는 LLS(Linear Least Squares) 로 부름
- 단순 선형 회귀에서 모델을 최적화하는 파라미터 값을 훈련 데이터로부터 학습하는 방법
- 비용 함수 : 손실함수. 모델의 오차를 정의하고 측정하기 위해 사용
- 잔차 : residual. 훈련 오차로 훈련 데이터의 관측 값과 모델에 의한 예측값의 차이
- 예측 오차 : preidction error. 테스트 오차로 테스트 데이터의 관측 값과 모델에 의한 예측값의 차이



### 비용함수를 사용한 모델의 적홥화 척도 : RSS

- 반응 변수의 예측값이 훈련 데이터의 관측값과 가까워지면 모델이 적합화 되었다고 할 수 있음
- 모델 적합화의 척도 : 잔차 제곱 합(RSS: Residual Sum of Squares)

## 다중 선형 회귀

- 다중 선형 회귀란 설명 변수인 다수의 특징과 단일 반응 변수 간에 선형 관계가 있다고 가정하고 초평면이라고 하는 선형 평면을 이용해 모델링한 것
- 다중 회귀는 설명 변수의 n차원과 반응 변수의 1차원, 모두 n+1개의 차원을 가지며, 초평면은 한차원 낮은 n차원이 됨

### 다중 선형 회귀의 벡터 정리

- 선형 회귀 모델의 벡터 표기법

> Y = XB

- Y : 반응 변수의 예측값을 가진 열 벡터
- B : 알고리즘을 통해 학습하게 되는 모델 파라미터 값을 갖는 열 벡터
- X : 설명 변수 값을 갖는 m X n 차원 행렬로 m은 훈련데이터의 개수, n은 특징의 개수



- 훈련 데이터로부터 X와 Y 의 값을 알고 있으므로 비용함수를 최소화하는 B를 계산
- 행렬 연산에서 나누니가 없으므로 역행렬을 사용해야 하고 역행렬을 사용하려면 역함수를 사용하는 정방행렬이어야 함
  

## 다항회귀

- 다항 회귀란 반응 변수와 다항식으로 표현된 특징 사이의 선형 관계를 모델링 할 수 있는 다중 선형회귀의 특수한 형태
- 특징을 변환한 후 다중 선형 회귀에서와 같은 방식으로 설명 변수와 반응 변수의 비선형 관계를 모델링함
- 단일 설명 변수의 단일 특징을 사용하고 있지만 모델의 항은 n개



## 주요 내용 정리

1. 선형회귀 모델을 생성하기에 앞서 상관분석, 산점도 행렬 등을 이용해 데이터에 대한 탐색적 분석을 실행
2. 단순 선형회귀란 설명 변수인 단일 특징과 단일 반응 변수간에 선형관계가 있다고 가정하고 초평면이라고 하는 선형 평면을 이용해 모델링 한 것이다.
3. 다중 선형회귀는 설명 변수인 다수의 특징과 단일 반응변수 간에 선형 관계가 있다고 가정하고 초평면이라고 하는 선형 평면을 이용해 모델링 한 것이다.
4. 다항 회귀는 설명 변수와 반응 변수의 비선형 관계를 모데링 한다.
5. 회귀모델 성능의 기본 평가지표는 결정계수 R제곱을 이용해 측정합니다.

