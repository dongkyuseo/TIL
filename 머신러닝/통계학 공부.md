# [정규분포](https://ko.wikipedia.org/wiki/%EC%A0%95%EA%B7%9C_%EB%B6%84%ED%8F%AC)

- [정규분포](https://www.youtube.com/watch?v=CQA7cdxozHY&ab_channel=classlive) : 평균과 표준편차가 주어져 있을 때 엔트로피를 최대화하는 분포

  중심극한정리에 따르면 평균이 u이고 분산이 시그마제곱(표준펀차가 시그마)인 모집단으로 부터 가능한 모든 n개의 조합을 표본으로 추출하면 표본의 평균은 정규분포에 접근함

- 표준정규분포 : 평균이 0, 표준편차가 1

- 편차, 분산, 상관도(상관계수)

- 분산(variance) : (평균과의 거리를 제곱한 값)의 평균, ; (평균-x)**2 의 합의 평균

- p-value : 귀무가설이 옳다는 전제 하에 표본에서 실제로 관측된 통계값과 같거나, 더 극단적인 통계값이 관측될 확률

- [트레이드오프(trade-off)](https://dsbook.tistory.com/141) : 머신러닝의 모델 임계값(Threshold)를 조정함으로써 정밀도나 재현율의 수치를 높일수 있으나 정밀도와 재현율은 상호 보완적 관계이기에, 한쪽을 강제로 높이면 한쪽은 떨어지게 됨. 이런 현상을 트레이드 오프라고 함

  - 정밀도 : 정답이라고 한것 중 실제 정답인 확률
  - 재현율 : 실제 정답중 정답인 확률

- [왜도와 첨도](https://yjam.tistory.com/90)

  - 왜도(Skewness) : (데이터 퍼짐의 정도) 데이터의 분포가 한쪽으로 쏠린것을 의미함, Skewed 되어있는 값을 그대로 학습시키면 꼬리부분이 상대적으로 모델에 영향이 거의 없이 학슴됨. 꼬리부분이 노이즈(이상치)가 아닌 유의미한 데이터이면 꼬리부분에 데이터는 예측력이 낮아짐(학습이 잘 안되기 때문) 변환을 하면 데이터의 중간값과 꼬리가 가까워져 모델에 보다 크게 영향을 줌
    - Positive Skewness : 왼쪽에 데이터가 많은 형태
    - Negative Skewness 오른쪽에 데이터가 많은 형태
  - 첨도(Kurtosis) : (데이터 쏠림의 정도) 첨도는 분포의 뾰족함이나 평평함에 관련된 것이 아닌, 꼬리에 대한 모든것을 의미함. 한쪽 꼬리부분의 극값과 다른쪽 꼬리의 극값 간의 차이를 보여줌(아웃라이어 찾을때 사용), 
    - 첨도가 높으면 아웃라이어가 많이 있음을 의미
    - 첨도가 낮으면 극값이 정규분포의 값보다 작기 떄문에 결과에 대한 확인이 필요

- [그리드 서치](https://ssoondata.tistory.com/30) : 관심 있는 매개변수들을 대상으로 가능한 모든 조합을 시도하여 최적의 매개변수를 찾는 방법으로 매개변수를 튜닝하여 일반화 성능을 개선해줌, 훈련 세트와 검증 세트에 교차 검증을 사용해서, 각 매개변수 조합의 성능을 평가하는 것이 일반화 성능을 잘 평가하는 방법임, 단 차원의 저주가 발생할 수 밖에 없음, 1개의 하이퍼파라미터를 결정하면 큰 문제가 없지만, 검색해야 하는 하이퍼파라미터의 수가 많아질수록 다수의 최적값을 찾는데 시간이 오래걸림

- [랜덤서치](https://optilog.tistory.com/7) : 그리드서치의 단점을 극복하기 위한 모델로 하이퍼파라미터 값을 무작위로 선정하여 그중 가장 좋은 값을 도출한 하이퍼파라미터를 최적값으로 봄, 이 방법론은 한정된 비용(탐색횟수)안에서 유용한 방법이며 차원의 저주에서 벗어날 수 있게 해줌. 랜덤서치는 탐색 초기에 유용하게 활용되나, 최적해를 찾는데 적합하지는 않음, 무작위로 결과를 뽑는점에서 샘플리오가 연결 지을 수 있고, 도출된 결과 값은 표본으로 활용되어 목표달성을 위한 힌트로 활용 가능함. 무작위 값이기에 최적인지 아닌지 판단할 방법이 없음, 때문에 아무런 힌트가 없는 초기에 유용한 방법임

  - [차원의 저주(The curse of dimensionality)](https://datapedia.tistory.com/15) : 차원이 증가함에 따라(변수의 증가) 모델의 성능이 안좋아지는 현상을 의미. 변수의 수가 증가해서 생기는 문제가 아닌, 관측치 수보다 변수의 수가 많아지면 발생함, KNN알고리즘이 차원의 저주에 가장 치명적인 알고리즘



## [중심극한 정리](https://drhongdatanote.tistory.com/57)

- 모집단 분포에 상관없이 큰 표본들의 표본평균 분포가 정규분포로 수렴한다는 점을 이용하여 Z값을 구해 확률값을 구할 수 있게 된다. 즉, 수학적 확률 판단(추정)을 할 수 있다.



## [난수 정규분포](https://ko.khanacademy.org/computing/computer-programming/programming-natural-simulations/programming-randomness/a/normal-distribution-of-random-numbers)

- [파이썬 난수 정규분포](https://rfriend.tistory.com/284)
- [파이썬 난수 정규분포2](https://codetorial.net/numpy/random.html)
- [텐서플로우 난수 정규분포](https://dschloe.github.io/python/tensorflow2.0/ch3_3_1_random_signoid/)
- 

## [데이터 시각화](https://rk1993.tistory.com/entry/Pythonseaborn-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%8B%9C%EA%B0%81%ED%99%94-regplot-lmplot-catplot-swarmplot)

