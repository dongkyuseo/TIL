# [언어모델](https://wikidocs.net/21668)

- 단어 시퀀스에 확률을 할당하는 모델
- 이전 단어들이 주어졌을때 다음 단어를 예측하도록 하는 것

## 단어 시퀀스의 확률 할당

- 기계번역

  - 나는버스를탔다나는버스를태운다

    P(나는 버스를 탔다) > P(나는 버스를 태운다)
    : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다.

- 오타교정

  - 선생님이 교실로 부리나케
    달려갔다잘려갔다P(달려갔다) > P(잘려갔다)
    : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다.

- 음성인식

  - 나는메롱을먹는다나는메론을먹는다

    P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)
    : 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단합니다.

## 주어진 이전 단어들로부터 다음 단어 예측하기

- 언어 모델은 단어 시퀀스에 확률을 할당하는 모델



## 통계적 언어 모델

### 조건부 확률



## N-gram 언어 모델

### N-gram

- unigrams
- bigrams : 2단어가 묶인 것
- trigrams : 3단어가 묶인 것
- 4-grams : 4 단어가 묶인 것
- N-gram은 단어뿐만 아닌 문자에서도 적용 됨
- n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존



## 한국어에서의 언어 모델

- 한국어는 어순이 중요하지 않다
- 한국어는 교착어이다 : 가령 '그녀'라는 단어 하나만 해도 그녀가, 그녀를, 그녀의, 그녀와, 그녀로, 그녀께서, 그녀처럼 등과 같이 다양한 경우가 존재합니다. 그렇기 때문에, 한국어에서는 **토큰화**를 통해 접사나 조사 등을 분리하는 것은 중요한 작업
- 한국어는 띄어쓰기가 제대로 지켜지지 않는다



## 퍼플렉서티

- 펄플렉서티(perplexity)는 언어 모델을 평가하기 위한 내부 평가 지표입니다. 보통 줄여서 PPL이 라고 표현
- PPL은 수치가 높으면 좋은 성능을 의미하는 것이 아니라, '낮을수록' 언어 모델의 성능이 좋다는 것을 의미



## Bag of Words

- 단어들의 순서는 고려하지 않고 단어들의 출현 빈도에만 집중하는 테스트 데이터 수치화 표현 방법
- 단어들의 출현빈도와 인덱스를 부여함

### CountVectorizer 클래스로 BoW 만들기

- 사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원
- 이를 이용하면 영어에 대해서는 손쉽게 BoW를 만들 수 있음
- 

